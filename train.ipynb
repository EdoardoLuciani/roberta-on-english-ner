{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizerFast, RobertaForTokenClassification, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "from tqdm.notebook import tqdm\n",
    "import torch, json, os, pandas as pd\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_fast = RobertaTokenizerFast.from_pretrained('roberta-base', add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/datasets/tner/ontonotes5\n",
    "ds = load_dataset(\"tner/ontonotes5\", keep_in_memory=True, num_proc=os.cpu_count())\n",
    "\n",
    "!wget -nc https://huggingface.co/datasets/tner/ontonotes5/resolve/main/dataset/label.json\n",
    "\n",
    "with open('label.json') as f:\n",
    "    ds_label_tag_mapping = json.load(f)\n",
    "    ds_tag_label_mapping = dict((v,k) for k,v in ds_label_tag_mapping.items())\n",
    "    assert len(ds_label_tag_mapping) == len(ds_tag_label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(input, idx):\n",
    "    tokenized_inputs = tokenizer_fast(input[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    input_ids = tokenized_inputs['input_ids']\n",
    "    word_ids: list[int] = tokenized_inputs.word_ids()\n",
    "    labels: list[int] = [0] * len(input_ids)\n",
    "\n",
    "    assert len(input_ids) == len(word_ids) == len(labels)\n",
    "\n",
    "    for i, word_id in enumerate(word_ids):\n",
    "        # word_id is none on first and last padding tokens which are automatically introduced by the tokenizer, setting those to -100\n",
    "        if word_id == None:\n",
    "            #assert i == 0 or i == (len(input_ids) - 1)\n",
    "            labels[i] = -100\n",
    "        # if this is a continuation of a previous word and the word has an actual meaning\n",
    "        elif word_id == word_ids[i-1] and input['tags'][word_id] != 0:\n",
    "            prev_word_tag: int = input['tags'][word_ids[i-1]]\n",
    "            prev_word_label: str = ds_tag_label_mapping[prev_word_tag]\n",
    "\n",
    "            if prev_word_label.startswith('I-'):\n",
    "                labels[i] = prev_word_tag\n",
    "            elif prev_word_label.startswith('B-'):\n",
    "                labels[i] = ds_label_tag_mapping[prev_word_label.replace('B-', 'I-')]\n",
    "            else:\n",
    "                raise Exception(f\"Cannot determine label for word_id {word_id} and dataset row {idx}\")\n",
    "        else:\n",
    "            labels[i] = input['tags'][word_id]\n",
    "\n",
    "    tokenized_inputs['labels'] = labels\n",
    "\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_entries(input, max_list_size=512):\n",
    "    input: dict = dict(input)\n",
    "    output: dict[str, list[int]] = {k:[[]] for k in input.keys()}\n",
    "\n",
    "    for x,y,z in zip(input['input_ids'], input['attention_mask'], input['labels']):\n",
    "        assert len(x) == len(y) == len(z)\n",
    "\n",
    "        if len(output['input_ids'][-1]) + len(x) < max_list_size:\n",
    "            output['input_ids'][-1].extend(x)\n",
    "            output['attention_mask'][-1].extend(y)\n",
    "            output['labels'][-1].extend(z)\n",
    "        else:\n",
    "            output['input_ids'].append(x)\n",
    "            output['attention_mask'].append(y)\n",
    "            output['labels'].append(z)\n",
    "\n",
    "\n",
    "    for x,y,z in zip(output['input_ids'], output['attention_mask'], output['labels']):\n",
    "        elem_diff = max_list_size - len(x)\n",
    "\n",
    "        x.extend(elem_diff * [1])\n",
    "        y.extend(elem_diff * [0])\n",
    "        z.extend(elem_diff * [-100])\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(split: str):\n",
    "    return (ds[split].map(tokenize_and_align_labels, with_indices=True, remove_columns=['tokens', 'tags'], num_proc=os.cpu_count())\n",
    "            .shuffle(seed=5473)\n",
    "            .map(group_entries, batched=True, batch_size=None))\n",
    "\n",
    "train_tokenized_dataset = process_dataset('train')\n",
    "validation_tokenized_dataset = process_dataset('validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = RobertaForTokenClassification.from_pretrained('roberta-base', num_labels=len(ds_label_tag_mapping))\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    return accuracy_score(labels, predictions)\n",
    "\n",
    "batch_size = 8\n",
    "training_steps = len(train_tokenized_dataset['input_ids']) * 3 / batch_size\n",
    "steps_per_actions = int(training_steps * 0.10)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"steps\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_steps=steps_per_actions,\n",
    "    eval_steps=steps_per_actions,\n",
    "    logging_steps=steps_per_actions\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized_dataset,\n",
    "    eval_dataset=validation_tokenized_dataset,\n",
    "    tokenizer=tokenizer_fast\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "roberta-traning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
